apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-inference-config
  namespace: ai-services
data:
  config.yaml: |
    server:
      port: 8000
      timeout: 30s
    
    observability:
      metrics:
        enabled: true
        port: 9090
        path: /metrics
      tracing:
        enabled: true
        exporter: otlp
        endpoint: http://otel-collector:4317
        sample_rate: 1.0
      logging:
        level: info
        format: json
    
    ai:
      models:
        - name: gpt-4
          provider: openai
          max_tokens: 4096
          temperature: 0.7
        - name: gpt-3.5-turbo
          provider: openai
          max_tokens: 4096
          temperature: 0.7
      
      caching:
        enabled: true
        ttl: 3600
        max_size: 1000
      
      rate_limiting:
        enabled: true
        requests_per_minute: 100
        burst: 20
